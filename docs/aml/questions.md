# Questions

## Deep Learning Essentials

- Why does the network not learn anything if linear functions?
- Why no activation function after every layer if this is crucial?
- What is gradient? Derivative of all? What is loss? Error? What does propagating loss mean? 
- How exactly works forward pass and backpropagation?
- Softmax function?
- In all the visualizations: what are the layers, where are the weights, where are the activation functions? Parameters?

